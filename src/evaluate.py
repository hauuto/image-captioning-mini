import torch
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from nltk.translate.bleu_score import corpus_bleu
from PIL import Image
from torchvision import transforms

# Import config ƒë·ªÉ l·∫•y c√°c th√¥ng s·ªë ·∫£nh
from src.config import Config

class Evaluator:
    def __init__(self, model, vocab, device):
        self.model = model
        self.vocab = vocab
        self.device = device
        self.model.eval() # Lu√¥n nh·ªõ b·∫≠t ch·∫ø ƒë·ªô eval

    def clean_sentence(self, indices):
        """
        Chuy·ªÉn list index th√†nh c√¢u ho√†n ch·ªânh, lo·∫°i b·ªè <SOS>, <EOS>, <PAD>
        """
        sentence = []
        for index in indices:
            word = self.vocab.itos.get(index, "<UNK>")
            if word == "<EOS>":
                break
            if word not in ["<SOS>", "<PAD>"]:
                sentence.append(word)
        return sentence

    def calculate_metrics(self, data_loader):
        """
        T√≠nh ƒëi·ªÉm BLEU-1,2,3,4 tr√™n to√†n b·ªô t·∫≠p test
        """
        print("üìä ƒêang t√≠nh to√°n BLEU score...")

        references = [] # Ground Truth (List of Lists)
        hypotheses = [] # Predictions (List)

        with torch.no_grad():
            for imgs, captions in tqdm(data_loader, desc="Evaluating"):
                imgs = imgs.to(self.device)

                # 1. Sinh caption (Generate)
                features = self.model.encoder(imgs)

                # Loop qua t·ª´ng ·∫£nh trong batch ƒë·ªÉ generate
                for i in range(imgs.size(0)):
                    # L·∫•y feature c·ªßa ·∫£nh i: [1, Embed_Size]
                    feature = features[i].unsqueeze(0)

                    # Generate ra list index
                    generated_ids = self.model.decoder.generate(feature, max_len=20, vocab=self.vocab)

                    # Chuy·ªÉn th√†nh list t·ª´ (words)
                    pred_caption = self.clean_sentence(generated_ids)
                    hypotheses.append(pred_caption)

                    # X·ª≠ l√Ω caption g·ªëc (Target)
                    target_ids = captions[i].tolist()
                    target_caption = self.clean_sentence(target_ids)

                    # Th√™m v√†o list reference (L∆∞u √Ω: corpus_bleu c·∫ßn list of lists cho reference)
                    references.append([target_caption])

        # 2. T√≠nh ƒëi·ªÉm b·∫±ng NLTK
        bleu1 = corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0))
        bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))
        bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))
        bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))

        print("\n" + "="*30)
        print(f"K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (BLEU SCORE)")
        print("="*30)
        print(f"üîπ BLEU-1: {bleu1*100:.2f}")
        print(f"üîπ BLEU-2: {bleu2*100:.2f}")
        print(f"üîπ BLEU-3: {bleu3*100:.2f}")
        print(f"üåü BLEU-4: {bleu4*100:.2f} (Quan tr·ªçng nh·∫•t)")
        print("="*30)

        return bleu4

    def visualize(self, data_loader, num_samples=6):
        """
        Hi·ªÉn th·ªã ·∫£nh v√† so s√°nh caption
        """
        print("üñºÔ∏è ƒêang t·∫°o ·∫£nh minh h·ªça...")
        imgs, captions = next(iter(data_loader))
        imgs = imgs.to(self.device)

        # Un-normalize ƒë·ªÉ hi·ªÉn th·ªã ·∫£nh ƒë√∫ng m√†u
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])

        plt.figure(figsize=(15, 10))

        for i in range(min(num_samples, imgs.size(0))):
            # 1. Generate
            with torch.no_grad():
                feature = self.model.encoder(imgs[i].unsqueeze(0))
                gen_ids = self.model.decoder.generate(feature, vocab=self.vocab)
                gen_cap = " ".join(self.clean_sentence(gen_ids))

            # 2. Ground Truth
            gt_cap = " ".join(self.clean_sentence(captions[i].tolist()))

            # 3. X·ª≠ l√Ω ·∫£nh
            img_np = imgs[i].cpu().permute(1, 2, 0).numpy()
            img_np = std * img_np + mean
            img_np = np.clip(img_np, 0, 1)

            # 4. V·∫Ω
            plt.subplot(2, 3, i + 1)
            plt.imshow(img_np)
            plt.title(f"Truth: {gt_cap}\nPred: {gen_cap}", fontsize=9, loc='left', color='blue')
            plt.axis("off")

        plt.tight_layout()
        plt.show()

    def predict_image(self, image_path):
        """
        D·ª± ƒëo√°n cho 1 file ·∫£nh b·∫•t k·ª≥ (.jpg, .png)
        """
        transform = transforms.Compose([
            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])

        try:
            image = Image.open(image_path).convert("RGB")
            # Clone ·∫£nh g·ªëc ƒë·ªÉ hi·ªÉn th·ªã
            original_image = image.copy()

            # Transform ƒë·ªÉ ƒë∆∞a v√†o model
            img_tensor = transform(image).unsqueeze(0).to(self.device)

            with torch.no_grad():
                caption = self.model.generate_caption(img_tensor, self.vocab)

            plt.figure(figsize=(6, 6))
            plt.imshow(original_image)
            plt.title(f"AI Caption: {caption}", fontsize=12, color='darkgreen', fontweight='bold')
            plt.axis("off")
            plt.show()

            print(f"üìù Caption: {caption}")
            return caption

        except Exception as e:
            print(f"‚ùå L·ªói: {e}")
            return None